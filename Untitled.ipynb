{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "from torchsummary import summary\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class capsLayer(nn.Module):\n",
    "    def __init__(self, num_route_nodes, num_capsules):\n",
    "        super(capsLayer, self).__init__()\n",
    "        self.num_route_nodes = num_route_nodes\n",
    "        self.num_capsules = num_capsules\n",
    "        if num_route_nodes != -1:\n",
    "            self.route_weights = nn.Parameter(torch.randn(num_capsules, num_route_nodes, in_channels, out_channels))\n",
    "        else:\n",
    "            self.capsules = nn.ModuleList([nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding ) for _ in range(num_capsules)])\n",
    "    \n",
    "    def squash(self, tensor, dim = -1):\n",
    "        squared_norm = (tensor**2).sum(dim = dim, keepdim = True)\n",
    "        scale = squared_norm / (1+squared_norm)\n",
    "        return scale*tensor / torch.sqrt(squared_norm)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        if self.num_route_nodes != -1:\n",
    "            pred_vectors = x[None,:,:,None,:] @ self.route_weights[:None, :,:]\n",
    "            logits = Variable(torch.zeros(*pred_vectors.size()), device = device)\n",
    "            for i in range(self.num_terations):\n",
    "                cc = softmax(logits, dim=2)\n",
    "                outputs = self.squash((cc*pred_vectors).sum(dim=2, keepdim=True))\n",
    "                \n",
    "                if i!= self.num_iterations -1:\n",
    "                    delta_logits = (pred_vectors*outputs).sum(dim=-1, keepdim=True)\n",
    "                    logits = logits+delta_logits\n",
    "        else:\n",
    "            outputs = [capsule(x).view(x.size(0), -1, 1) for capsule in self.capsules]\n",
    "            outputs = torch.cat(outputs, dim=-1)\n",
    "            outputs = self.squash(outputs)\n",
    "            \n",
    "        return outputs\n",
    "                \n",
    "\n",
    "            \n",
    "            outputs = self.squash((probs*priors))\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (8) must match the size of tensor b (39) at non-singleton dimension 1",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-89-ed4f02cabb1a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m128\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m8\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m54\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m33\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mv\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m8\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m39\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m33\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m66\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mz\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mn\u001b[0m\u001b[1;33m@\u001b[0m\u001b[0mv\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[0mn\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m@\u001b[0m \u001b[0mv\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: The size of tensor a (8) must match the size of tensor b (39) at non-singleton dimension 1"
     ]
    }
   ],
   "source": [
    "# TESTING\n",
    "# n = torch.randn(128,8,54,33)\n",
    "# v = torch.randn(8, 39, 33, 66)\n",
    "# z = n@v\n",
    "# n=n[None, :, :, None, :] @ v[:None, :,:,:]\n",
    "# n.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 2, 3, 4])\n",
      "torch.Size([2, 1, 3, 4])\n"
     ]
    }
   ],
   "source": [
    "n = torch.randn(1, 2, 3, 4)\n",
    "print(n.shape)\n",
    "n = n.transpose(0, 1)\n",
    "print(n.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "class capsNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(capsNet, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels = 3, out_shannels = 256, kernel_size = 9, stride = 1)\n",
    "        \n",
    "        self.primary_capsules = capsLayer(num_capsules = 8, num_route_nodes = -1, in_channels = 256, out_channels =32, kernel_size = 9, stride = 2)\n",
    "        self.digit_capsules = capsLayer(num_capsules=10, num_route_nodes = 32*6*6, in_channels = 8, out_channels = 16)\n",
    "        self.decoder = nn.Sequential(nn.Linear(16*10, 512),\n",
    "                                    nn.ReLU(inplace = True),\n",
    "                                    nn.Linear(512, 1024),\n",
    "                                    nn.ReLU(inplace = True),\n",
    "                                    nn.Linear(1024, 784),\n",
    "                                    nn.sigmoid()\n",
    "                                    )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.conv1(x), inplace = True)\n",
    "        print(out.shape)\n",
    "        x = self.primary_capsules(x).squeeze().transpose(0, 1)\n",
    "        x = self.digit_capsules(x).squeeze().transpose(0, 1)   \n",
    "        \n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 258, 12, 12])\n",
      "torch.Size([2, 128, 1])\n",
      "torch.Size([2, 128, 1])\n",
      "torch.Size([2, 128, 1])\n",
      "torch.Size([2, 128, 1])\n",
      "torch.Size([2, 128, 1])\n",
      "torch.Size([2, 128, 1])\n",
      "torch.Size([2, 128, 1])\n",
      "torch.Size([2, 128, 1])\n",
      "torch.Size([128, 8])\n",
      "torch.Size([128, 8])\n",
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1          [-1, 258, 12, 12]          62,952\n",
      "            Conv2d-2             [-1, 32, 2, 2]         668,768\n",
      "            Conv2d-3             [-1, 32, 2, 2]         668,768\n",
      "            Conv2d-4             [-1, 32, 2, 2]         668,768\n",
      "            Conv2d-5             [-1, 32, 2, 2]         668,768\n",
      "            Conv2d-6             [-1, 32, 2, 2]         668,768\n",
      "            Conv2d-7             [-1, 32, 2, 2]         668,768\n",
      "            Conv2d-8             [-1, 32, 2, 2]         668,768\n",
      "            Conv2d-9             [-1, 32, 2, 2]         668,768\n",
      "================================================================\n",
      "Total params: 5,413,096\n",
      "Trainable params: 5,413,096\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 0.29\n",
      "Params size (MB): 20.65\n",
      "Estimated Total Size (MB): 20.95\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "model = capsNet()\n",
    "summary(model, (3, 20, 20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torchsummary\n",
      "  Downloading https://files.pythonhosted.org/packages/7d/18/1474d06f721b86e6a9b9d7392ad68bed711a02f3b61ac43f13c719db50a6/torchsummary-1.5.1-py3-none-any.whl\n",
      "Installing collected packages: torchsummary\n",
      "Successfully installed torchsummary-1.5.1\n"
     ]
    }
   ],
   "source": [
    "!pip install torchsummary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
